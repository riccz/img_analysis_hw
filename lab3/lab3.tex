\documentclass[a4paper,oneside]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage[margin=2.54cm]{geometry}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage[backend=biber,style=ieee]{biblatex}

\usepackage{customcmds}
\usepackage{codecmds}

\addbibresource{lab3.bib}

\DeclareMathOperator{\clip}{clip}

\author{Riccardo Zanol}
\title{Laboratory 3}

\begin{document}
\matlabcodeconfig
\maketitle
\section*{Experiment 1} \stepcounter{section}
The purpose of the first part of this homework is to implement two
demosaicing algorithms and test them, together with the one
implemented in Matlab, on the four images \inlinecode{kodimXX.png} to
compare their performances, visually and using the mean squared error
in the CIELAB color space.

Each one of the test images is downsampled by the provided function
\inlinecode{create_bayer} according to the GRBG Bayer mask (see
Fig.~\ref{fig:bayer_grbg}), in order to simulate the kind of data that
would be acquired by a single-sensor imaging system,
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.66\textwidth]{include_imgs/bayer_grbg}
    \caption{GRBG}
    \label{fig:bayer_grbg}
  \end{subfigure}%
  \begin{subfigure}{0.5\textwidth}
    \centering
    \includegraphics[width=0.66\textwidth]{include_imgs/bayer_rggb}
    \caption{RGGB}
    \label{fig:bayer_rggb}
  \end{subfigure}
  \caption{The two Bayer CFA patterns used in this homework}
\end{figure}
and is then demosaiced using three of the algorithms described
in~\cite{demosaic_alg}:~the linear interpolation, the
gradient-corrected linear interpolation and a combination of the
pattern recognition interpolation and the smooth hue transition
interpolation.

\subsection{Linear interpolation}
\label{sec:linear}
The linear interpolation algorithm is the simplest of the three, it
just sets the missing color components of each pixel to their means in
a 3~by~3 window around the pixel.
%
As an example, in Fig.~\ref{fig:bayer_neigh}, the color of the central
pixel is set to
\begin{align*}
  R_5 &= R_5 \\
  G_5 &= \frac{1}{4}\left( G_2 + G_4 + G_6 + G_8 \right) \\
  B_5 &= \frac{1}{4}\left( B_1 + B_3 + B_7 + B_9 \right) .
\end{align*}
When the algorithm computes the means of the red and blue components
around a green pixel, e.g.~$R_6$~and~$B_6$ in
Fig.~\ref{fig:bayer_neigh}, it takes into account only the two pixels
of the corresponding color that are present in the 3~by~3 window so
\begin{equation*}
  B_6 = \frac{1}{2} \left( B_3 + B_9 \right)
\end{equation*}
and $R_6$ is the mean between $R_5$ and the value at the right of
pixel~6.
%
The same thing also occurs near the picture borders, where there are
only three or two pixels to interpolate the green values and just two
or one pixel to interpolate the other colors.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.20\textwidth]{include_imgs/bayer_neigh}
  \caption{Neighborhood of a red pixel in a Bayer pattern}
  \label{fig:bayer_neigh}
\end{figure}

This algorithm is implemented by the function
\inlinecode{demosaic_linear} and the output images are saved as
\inlinecode{demosaic_kodimXX.png}.

\subsection{Gradient-corrected linear interpolation}
The gradient-corrected linear interpolation is a more complex
algorithm, described in detail in \cite{demosaic_gradient}, that
exploits the correlation between the color components to improve the
quality of the demosaiced image.
%
The mean values of the neighbors, $\hat{r}_{lin}$, $\hat{g}_{lin}$ and
$\hat{b}_{lin}$ computed like in the previous algorithm, are corrected
using the gradient of the color that was measured at that location:
\begin{align*}
  \hat{g}(i,j) &= \hat{g}_{lin}(i,j) + \alpha \Delta_R(i,j) & & \text{red pixel location} \\
  \hat{g}(i,j) &= \hat{g}_{lin}(i,j) + \alpha \Delta_B(i,j) & & \text{blue pixel location} \\
  %
  \hat{r}(i,j) &= \hat{r}_{lin}(i,j) + \beta \Delta_G(i,j) & & \text{green pixel location} \\
  \hat{r}(i,j) &= \hat{r}_{lin}(i,j) + \beta \Delta_B(i,j) & & \text{blue pixel location} \\
  %
  \hat{b}(i,j) &= \hat{b}_{lin}(i,j) + \gamma \Delta_G(i,j) & & \text{green pixel location} \\
  \hat{b}(i,j) &= \hat{b}_{lin}(i,j) + \gamma \Delta_R(i,j) & & \text{red pixel location} \\
\end{align*}
where the gradients are computed on a 5~by~5 window around the pixel
at position $(i,j)$ using a mask that depends on the location and on
the color component that is being interpolated. As an example, when
the green value is interpolated at the location of a red pixel, the
gradient is computed as:
\begin{equation*}
  \Delta_R(i,j) \triangleq r(i,j) - \frac{1}{4} \sum_{(m,n)} r(i+m,j+n)
  \qquad (m,n) \in \{(0,-2), (0,2), (-2,0), (2,0)\} .
\end{equation*}
The gradient-correction gains are chosen in order to minimize the mean
squared error given the statistics of an image set (which contains the
test images used in this homework) and they are approximated to
multiples of small powers of $1/2$: $\alpha = 1/2$, $\beta = 5/8$ and
$\gamma = 3/4$. In \cite{demosaic_gradient} the eight possible masks
for the computation of the gradient are scaled by the
gradient-correction gains and substituted in the six equations above
to obtain eight linear FIR filters to apply to the image.

This algorithm is the one implemented by the built-in Matlab function
\inlinecode{demosaic} and the output images are saved as
\inlinecode{matlab_demosaic_kodimXX.png}.

\subsection{Pattern recognition and smooth hue transition interpolation}
Another demosaicing algorithm described briefly in \cite{demosaic_alg}
and in more detail in \cite{demosaic_army} is the pattern recognition
interpolation, which classifies the pixels into edges, corners and
stripes according to their neighborhood and applies different
interpolation rules for each kind of point.
%
It considers the four neighbors of each missing green pixel and
computes their mean, then labels each neighbor as either ``low'', when
its value is smaller than the mean, or ``high'' otherwise.
%
These labels are used to classify the missing pixel in one of the
categories mentioned above:
\begin{itemize}
\item if three neighbors share the same label the pixel is part of an
  edge (see Fig.s \ref{fig:lowedge} and \ref{fig:highedge}),
\item if two adjacent neighbors share the same label the pixel is part
  of a corner (see Fig. \ref{fig:corner}),
\item
  if two opposite neighbors share the same label the pixel is part of
  a stripe (see Fig. \ref{fig:stripe}).
\item The only missing case, four neighbors with the same intensity, is
  handled like an edge by the code because, as will be seen in the
  next paragraph, this will give the interpolated pixel the median
  value of its neighbors (and so their same value).
\end{itemize}
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{include_imgs/pri_corner}
    \caption{Corner}
    \label{fig:corner}
  \end{subfigure}%
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{include_imgs/pri_lowedge}
    \caption{Low edge}
    \label{fig:lowedge}
  \end{subfigure}%
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{include_imgs/pri_highedge}
    \caption{High edge}
    \label{fig:highedge}
  \end{subfigure}%
  \begin{subfigure}{0.2\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{include_imgs/pri_stripe}
    \caption{Stripe}
    \label{fig:stripe}
  \end{subfigure}
  \caption{Example of classification done by the pattern recognition
    interpolation algorithm}
\end{figure}

After the pixels have been classified, the pattern recognition
algorithm sets their value according to their class:
\begin{itemize}
\item Edge points of both kinds are given the median value of their
  neighbors,
  \item Corner points are given the value $G = \clip_C^B(2M-S)$ where
    $S$ is the mean value of the pixels labeled ``X'' in
    Fig. \ref{fig:mask_corner},
    \item Stripe points are given the value $G = \clip_C^B(2M-S)$ where
    $S$ is the mean value of the pixels labeled ``X'' in
    Fig. \ref{fig:mask_stripe}.
\end{itemize}
In both the stripe and the corner cases above $M$ is the median of the
four closest neighbors, $B$ and $C$ are the values of the second and
the third neighbors taken in descending order ($A \geq B \geq C \geq
D$) and the $\clip$ function is defined as:
\begin{equation*}
  \clip_C^B(x) = \begin{cases}
    B \qquad & x > B \\
    x \qquad & C \leq x \leq B \\
    C \qquad & x < C
    \end{cases} .
\end{equation*}
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{include_imgs/pri_corner_mask}
    \caption{Corner}
    \label{fig:mask_corner}
  \end{subfigure}%
  \begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{include_imgs/pri_stripe_mask}
    \caption{Stripe}
    \label{fig:mask_stripe}
  \end{subfigure}
  \caption{Masks used in the computation of the interpolated value}
\end{figure}

There is no mention on \cite{demosaic_alg, demosaic_army} on what the
algorithm should do near the picture border, when there are not enough
neighbors, however this is just a small two-pixel border so it should
not affect the performances so much.
%
If the four closest neighbors are not available the pixel cannot be
classified so the code uses the same linear interpolation algorithm of
section \ref{sec:linear}.
%
When, instead, the pixel can be classified but the ``X'' pixels of
Fig.s \ref{fig:mask_corner} and \ref{fig:mask_stripe} are not all
available, the mean $S$ is computed using only the ones that fall
inside the picture.

This algorithm is designed to work only on the green component so, in
order to perform a complete demosaicing of an image, another algorithm
must be used to interpolate the other colors. A suggestion given by
\cite{demosaic_alg, demosaic_army} is to use the smooth hue transition
interpolation algorithm, which assumes that the hue changes smoothly
across an object's surface and so it uses the ratio between the blue
or red components and the green component instead of their absolute
values.
%
The red-green and blue-green ratios are linearly interpolated, with
the same rules of section \ref{sec:linear}. As an example, the blue
component is interpolated in Fig.~\ref{fig:bayer_neigh} using
\begin{align*}
  B_2 &= G_2 \frac{1}{2} \left( \frac{B_1}{G_1} + \frac{B_3}{G_3} \right) \\
  B_4 &= G_4 \frac{1}{2} \left( \frac{B_1}{G_1} + \frac{B_7}{G_7} \right) \\
  B_5 &= G_5 \frac{1}{4} \left( \frac{B_1}{G_1} + \frac{B_3}{G_3} +
  \frac{B_7}{G_7} +\frac{B_9}{G_9}\right) .
\end{align*}
This method fails if some green values are zero, but the
transformation
\begin{equation*}
  \frac{B}{G} \rightarrow \frac{B + \beta}{ G + \beta} \qquad \beta > 0
\end{equation*}
can be used to avoid this issue.
%
With this transformation the previous equations become
\begin{align*}
  B_2 &= -\beta + \left(G_2 +\beta \right) \frac{1}{2} \left(
  \frac{B_1 + \beta}{G_1 + \beta} + \frac{B_3 + \beta}{G_3 + \beta} \right) \\
  B_4 &= -\beta + \left( G_4 + \beta \right) \frac{1}{2} \left(
  \frac{B_1 + \beta}{G_1 + \beta} + \frac{B_7 + \beta}{G_7 + \beta} \right) \\
  B_5 &= -\beta + \left( G_5 +\beta \right) \frac{1}{4} \left(
  \frac{B_1 + \beta}{G_1 + \beta} + \frac{B_3 + \beta}{G_3 + \beta} +
  \frac{B_7 + \beta}{G_7 + \beta} +\frac{B_9 + \beta}{G_9 + \beta}\right) .
\end{align*}
The value of $\beta$ that is suggested by \cite{demosaic_army} is
$\beta = 128$.

The pattern recognition interpolation and the smooth hue transition
interpolation algorithms are implemented, respectively, in the
functions \inlinecode{pattern_interp} and \inlinecode{smooth_hue} and
used together by the function \inlinecode{demosaic_pattern_hue} to
perform the demosaicing. The output images are saved as
\inlinecode{prsh_demosaic_kodimXX.png}.

\subsection{Comparison}
After the images have been demosaiced by each one of the three
algorithms, they are converted from the sRGB to the CIELAB color space
and the MSE between the output and the original images is
computed. The MSE between $A$ and $B$ is computed by taking the
euclidean distance between the colors of each corresponding pixel and
averaging over the image:
\begin{align*}
  d(i,j) &= \sqrt{\sum_{k=1}^3 \left(A(i,j,k) - B(i,j,k)\right)^2 } \\
  \text{MSE} &= \frac{1}{WH}\sum_{i=0}^H\sum_{j=0}^Wd(i,j)
\end{align*}
where $A(i,j,1) = L_A(i,j)$, $A(i,j,2) = a_A(i,j)$ and $A(i,j,3) =
b_A(i,j)$ are the three coordinates in the CIELAB space of the pixel
$(i,j)$ of image $A$.
%
The computed values of the MSE are shown in Tab.~\ref{tab:mse} where
it can be seen that the gradient-corrected linear interpolation is,
with respect to the MSE, always the best algorithm followed by the
pattern recognition and smooth hue transition interpolation
algorithms, while the linear interpolation always gives the highest
MSE.
\begin{table}[h]
  \centering
  \begin{tabular}{IRRR}
    \multicolumn{1}{c}{Image} &
    \multicolumn{1}{c}{Linear} &
    \multicolumn{1}{c}{Gradient-corrected} &
    \multicolumn{1}{c}{PRI and SHTI} \\
    \hline
    kodim01.png & 5.053 & 2.909 & 3.434 \\
    kodim05.png & 4.637 & 2.407 & 3.242 \\
    kodim13.png & 6.275 & 3.707 & 4.850 \\
    kodim19.png & 3.333 & 1.974 & 2.236 \\
  \end{tabular}
  \caption{Mean squared error of the three tested demosaicing
    algorithms }
  \label{tab:mse}
\end{table}

A visual comparison of the demosaiced images with the original,
however, shows that the MSE is not always a good indicator of the
performances of the algorithms (see Fig.s \ref{fig:wall} and
\ref{fig:door}).
%
The linear interpolation algorithm is the worst performing one, since
it creates a lot of artifacts in the output images, especially along
the edges. It also slightly blurs the image because it interpolates
the pixels on the edges using samples from both sides.
%
The gradient-corrected interpolation works better and the resulting
image is less blurred, but it still leaves very noticeable zippering
and false color artifacts.
%
The pattern recognition algorithm is designed specifically to avoid
interpolating across an edge so, as expected, it reduces a lot these
two kind of artifacts, even if it is not able to completely eliminate
them.
%
It also alters some small details, like the door knob in
Fig. \ref{fig:door} that becomes a square, and this may be the cause
for its higher MSE value than the gradient-corrected algorithm, even
if the resulting images look closer to the original ones.
%
Another reason for the mismatch between the MSE comparison and the
visual comparison may be that these particular images were part of the
set used to optimize the gradient-correction gain, so if new images
were used they would maybe yield worse MSE values.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/kodim19_wall}
    \caption{Original}
  \end{subfigure}%
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/demosaic_kodim19_wall}
    \caption{Linear interpolation}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/matlab_demosaic_kodim19_wall}
    \caption{Gradient-corrected linear interpolation}
  \end{subfigure}%
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/prsh_demosaic_kodim19_wall}
    \caption{PR and SHT interpolation}
  \end{subfigure}
  \caption{Comparison of some artifacts in \inlinecode{kodim_19.png}
    introduced by the demosaicing}
  \label{fig:wall}
\end{figure}
Another disadvantage of the demosaicing performed with the pattern
recognition and the smooth hue transition interpolation is that they
are non-linear (unlike the other two approaches) and require more
computational resources.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/kodim01_door}
    \caption{Original}
  \end{subfigure}%
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/demosaic_kodim01_door}
    \caption{Linear interpolation}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/matlab_demosaic_kodim01_door}
    \caption{Gradient-corrected linear interpolation}
  \end{subfigure}%
  \begin{subfigure}{0.4\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{include_imgs/generated/prsh_demosaic_kodim01_door}
    \caption{PR and SHT interpolation}
  \end{subfigure}
  \caption{Comparison of some artifacts in \inlinecode{kodim_01.png}
    introduced by the demosaicing}
  \label{fig:door}
\end{figure}

\section*{Experiment 2} \stepcounter{section}
The second part of this homework will implement a simple pipeline,
described in \cite{rawguide}, to process the RAW images captured by a
digital reflex camera and obtain a final image similar to what the
camera saves as JPEG file.

\subsection{Linearization}
First the pixel intensities must be linearized, because the camera may
perform a non-linear transformation for storage reasons, and rescaled
to fit the range of the pixel values. The linearization table is
provided by the camera in the image metadata and the
\inlinecode{read_dng} function just has to map the values using
it. The metadata also contains the black and the saturation value for
the sensor, which are used to map the pixel intensities in the $[0,1]$
interval.

\subsection{White Balancing}
Since the color sensed by the camera depends on the illuminant, and
also because the color filter array attenuates differently each of
the three color components, each channel must be scaled to balance the
white in the image.
%
This is done by arbitrarily setting one of the scale factors to 1
(usually the green one) and choosing the other two in order to make
certain known points of the image white (or gray). The camera stores
in the RAW metadata its best guess for the multipliers so the
\inlinecode{read_dng} function just applies them.

\subsection{Demosaicing}
Since the raw data of the image is stored according to the RGGB Bayer
pattern (see Fig.~\ref{fig:bayer_rggb}), instead of the GRBG used in
the previous experiment, the provided \inlinecode{read_dng} function
is modified to return the pixels from the whole active area and the
coordinates to crop the image separately. In this way the demosaicing
functions do not need to be modified: it is sufficient to drop the
first column from the image (and adjust the cropping coordinates
accordingly) to fall back to the GRBG case. The lost pixels do not
affect the final image because the active area is, in this case, 16
pixels wider and higher than the size the output is cropped to.

After having been demosaiced using each one of the three algorithms
described in the previous experiment: linear interpolation,
gradient-corrected interpolation and pattern recognition with smooth
hue transition interpolation, the images are cropped to their default
size, specified in the RAW metadata (6000 by 4000 pixels, starting
from $(7,8)$).

\subsection{Color space conversion}
The color of the image is, at this point, still expressed in terms of
the color space of the camera's sensor. So, to be visualized correctly
on a screen, the image must be converted into the sRGB color space.
%
In the RAW metadata there is the matrix to convert from the XYZ color
space to the camera's color space $A_{cam \leftarrow XYZ}$, that
combined with the sRGB to XYZ transformation matrix
\begin{equation*}
  A_{XYZ \leftarrow sRGB} = \left[ \begin{array}{ccc}
      0.4124564 & 0.3575761 & 0.1804375 \\
      0.2126729 & 0.7151522 & 0.0721750 \\
      0.0193339 & 0.1191920 & 0.9503041
    \end{array} \right]
\end{equation*}
gives the matrix $A_{cam \leftarrow sRGB} = A_{cam \leftarrow XYZ}
A_{XYZ \leftarrow sRGB}$ that converts from sRGB to the camera's color
space.
%
Since the white pixels must remain white after the transformation
(because the white balance was already performed) the transformation
matrix' rows must be normalized so that they sum to 1. With this
normalization
\begin{equation*}
  \left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right]_{cam} =
  A'_{cam \leftarrow sRGB}
  \left[ \begin{array}{c} 1 \\ 1 \\ 1 \end{array} \right]_{sRGB}
\end{equation*}
and the inverse of the normalized matrix
\begin{equation*}
  A'_{sRGB \leftarrow cam}  = \left( A'_{cam \leftarrow sRGB} \right)^{-1}
\end{equation*}
can be applied to the demosaiced images to perform the color
correction.

The provided function \inlinecode{read_dng} was modified to read and
return the $A_{cam \leftarrow XYZ}$ matrix, while the color correction
was added to the provided function \inlinecode{post_process}.

\subsection{Brightness and gamma correction}
After the color correction \inlinecode{post_process} applies a simple
brightening measure: it finds the mean luminance of the image and
scales the image intensities so that the mean luminance is $1/4$ of
the maximum.

Finally, a gamma correction with $\gamma = \frac{1}{2.2}$ is applied
to the image, to brighten the dark areas.

\subsection{Comparison}
In Fig. \ref{fig:chart} it can be seen that the output of the pipeline
looks correct enough, however the camera is likely using different
parameters and algorithms to process the RAW files, especially for the
last two steps: the brightening and the gamma correction.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/macbeth_color_small}
    \caption{JPEG produced by the camera}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/demosaic_macbeth_color_small}
    \caption{Linear demosaicing}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/matlab_demosaic_macbeth_color_small}
    \caption{Gradient-corrected demosaicing}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/prsh_demosaic_macbeth_color_small}
    \caption{PRI and SHTI demosaicing}
  \end{subfigure}
  \caption{Comparison of the color chart}
  \label{fig:chart}
\end{figure}
The choice of demosaicing algorithm does not appear to have any effect
of the final result and there are not many noticeable artifacts in the
images. Some of them, like those on the edges of the white car in
Fig. \ref{fig:park}, may also be caused by the lenses since they are
very close to the border of the picture.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/DEI_park_small}
    \caption{JPEG produced by the camera}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/demosaic_DEI_park_small}
    \caption{Linear demosaicing}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/matlab_demosaic_DEI_park_small}
    \caption{Gradient-corrected demosaicing}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/prsh_demosaic_DEI_park_small}
    \caption{PRI and SHTI demosaicing}
  \end{subfigure}
  \caption{Comparison of a detail from \inlinecode{DEI_park.dng}, near
    the picture border}
  \label{fig:park}
\end{figure}
Something that is very noticeable is the noisiness of the resulting
images, compared to the ones processed by the camera (see
Fig. \ref{fig:students}): the camera is likely applying some kind of
low-pass filter somewhere in the pipeline to smooth out the noise,
while the code implemented for this homework does not do so.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/students2_small}
    \caption{JPEG produced by the camera}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/demosaic_students2_small}
    \caption{Linear demosaicing}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/matlab_demosaic_students2_small}
    \caption{Gradient-corrected demosaicing}
  \end{subfigure}%
  \begin{subfigure}{.4\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{include_imgs/generated/prsh_demosaic_students2_small}
    \caption{PRI and SHTI demosaicing}
  \end{subfigure}
  \caption{Comparison of a dark area from \inlinecode{students2.dng}}
  \label{fig:students}
\end{figure}
The high noise level may be the reason that makes the choice of
demosaicing algorithm irrelevant, since the artifacts that are
introduced by the three tested algorithms are not distinguishable
from it.

\printbibliography
\end{document}
